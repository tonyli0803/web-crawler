{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自由時報\n",
    "\n",
    "import scrapy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "from argparse import ArgumentParser\n",
    "import datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "########判斷要不要更新資料#########\n",
    "###################################\n",
    "\n",
    "###上一次開始爬的新聞的相關資料\n",
    "date_last_time = ''\n",
    "time_last_time = ''\n",
    "url__last_time  = ''\n",
    "with open(\"ltn_news_timestamp.txt\") as f:\n",
    "    tmp = []\n",
    "    for i in f.readlines():\n",
    "        tmp.append(i.strip('\\n'))\n",
    "    if len(tmp)>0:\n",
    "        date_last_time = tmp[1].split('/')[-3]\n",
    "        time_last_time = tmp[0]\n",
    "        url__last_time = tmp[1]\n",
    "    else:\n",
    "        date_last_time = \"\"\n",
    "        time_last_time = \"\"\n",
    "        url__last_time = \"\"\n",
    "    #print(date_last_time,time_last_time,url__last_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "today = now.strftime(\"%Y-%m-%d \")\n",
    "#print(today)\n",
    "\n",
    "input_url = \"http://news.ltn.com.tw/list/breakingnews/all/1\"\n",
    "stop = False\n",
    "\n",
    "links = []\n",
    "timestamp = []\n",
    "\n",
    "stop = False\n",
    "cnt = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tony\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\tony\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 2\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 3\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 4\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 5\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 6\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 7\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 8\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 9\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 10\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 11\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 12\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 13\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 14\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 15\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 16\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 17\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 18\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 19\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 20\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 21\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 22\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 23\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 24\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 25\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 26\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 27\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 28\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 29\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 30\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 31\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 32\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 33\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 34\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 35\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 36\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 37\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 38\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 39\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 40\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 41\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 42\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 43\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 44\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 45\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 46\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 47\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 48\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n",
      "page 49\n",
      "-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in range(1,50):\n",
    "    try:\n",
    "        \n",
    "        # search each url in each page!!!\n",
    "        print(\"page %d\"%i)\n",
    "        res = BeautifulSoup(requests.get(input_url+str(i)).text)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for news in res.select('.list.imm')[0].select('.tit'):\n",
    "            #print(news)\n",
    "            \n",
    "            \n",
    "            title = news.select('p')[0].text\n",
    "            \n",
    "            #date = (news.select('a')[0]['href']).split('/')[-3]\n",
    "            date = news.select('span')[0].text\n",
    "            if len(date) < 11:\n",
    "                date = today + date\n",
    "\n",
    "            #time = news.select('a')[0].select('time')[0].text\n",
    "            url = news['href']\n",
    "            \n",
    "            \n",
    "            #print(\"--------\\n\")\n",
    "            #print(title)\n",
    "            #print(date,\"    \",len(date))\n",
    "            #print(url)\n",
    "            #print(\"--------\\n\")\n",
    "\n",
    "            ###判斷要不要從第一頁第一個項目開始爬\n",
    "            \n",
    "            #print( time,time_last_time,  time > time_last_time ,date == date_last_time and time_last_time < time)\n",
    "\n",
    "            '''\n",
    "            if date < date_last_time:\n",
    "                print(\"The data is up to date!!!!\\nUpdate %d data this time\"%cnt)\n",
    "                stop = True\n",
    "                break\n",
    "            elif date == date_last_time and time < time_last_time:\n",
    "                print(\"The data is up to date!!!!\\nUpdate %d data this time\"%cnt)\n",
    "                stop = True\n",
    "                break\n",
    "            elif date == date_last_time and time == time_last_time and url == url__last_time:\n",
    "                print(\"The data is up to date!!!!\\nUpdate %d data this time\"%cnt)\n",
    "                stop = True\n",
    "                break\n",
    "            #爬了多少筆資料的counter\n",
    "            cnt = cnt + 1\n",
    "\n",
    "\n",
    "            if args.filter != \"\":\n",
    "                if date <= args.filter:\n",
    "                    stop = True\n",
    "\n",
    "                    break\n",
    "\n",
    "            links.append(url)\n",
    "\n",
    "            print(\"----------------\\nScapying news in\")\n",
    "            print(date+\"  \"+time)\n",
    "            print(url)\n",
    "            \n",
    "            ###記錄這次的是從哪一個新聞開始爬起\n",
    "            if len(timestamp) == 0 : \n",
    "                timestamp.append(time)\n",
    "                timestamp.append(url)\n",
    "\n",
    "            #這是跑到該網站裡面取標題，如果加這個會慢一些\n",
    "            #tmp_res = BeautifulSoup(requests.get(tmp).text)\n",
    "            #print(tmp_res.select('h1')[0].text)\n",
    "            '''\n",
    "        \n",
    "            \n",
    "        if stop == True:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"tony log:: maybe reach the end of pages of the website\")\n",
    "    \n",
    "    last_page_flag = res.select('.p_last')\n",
    "    if last_page_flag == []:\n",
    "        break;\n",
    "    \n",
    "    \n",
    "    print(\"-*-*-*-*-*-*-*-*-*-*--*-**-*--*--*-*-*\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-77-21fa782bd1dd>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-77-21fa782bd1dd>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    #exit(1)\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#存下這次的url記錄\n",
    "#包括最新的url 其時間為何，以及有哪些URL等一下要爬\n",
    "if len(links)>0:\n",
    "\n",
    "    with open(\"apple_news_links.txt\",'w') as f:\n",
    "        for i in links:\n",
    "            f.write(i)\n",
    "            f.write('\\n')\n",
    "\n",
    "    with open(\"apple_news_timestamp.txt\",'w') as f:\n",
    "        for i in timestamp:\n",
    "            f.write(i)\n",
    "            f.write('\\n')\n",
    "    #exit(0)\n",
    "else:\n",
    "    #exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "國際\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "\n",
    "res = requests.get(\"http://news.ltn.com.tw/news/world/breakingnews/2533627\").text\n",
    "\n",
    "#test xpath\n",
    "selector = etree.HTML(res)\n",
    "#抓取新聞的類別  \n",
    "news_class = selector.xpath('/html/body/div[3]/section/div[3]/a[3]')\n",
    "print(news_class[0].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
